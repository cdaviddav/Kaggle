{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models with Advanced Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.4.3\n",
      "numpy version: 1.21.5\n",
      "sklearn version: 1.1.1\n",
      "xgboost version: 1.6.1\n",
      "lightgbm version: 3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programs\\anaconda3\\envs\\mlflow_optuna\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optuna version: 2.10.1\n",
      "mlflow version: 1.28.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "# numpy: support for large, multi-dimensional arrays and matrices and high-level mathematical functions\n",
    "import numpy as np\n",
    "print(\"numpy version: {}\". format(np.__version__))\n",
    "\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit, train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "print(\"sklearn version: {}\". format(sklearn.__version__))\n",
    "sklearn .set_config(display=\"diagram\")\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "print(\"xgboost version: {}\". format(xgboost.__version__))\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "print(\"lightgbm version: {}\". format(lightgbm.__version__))\n",
    "\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "import optuna\n",
    "print(\"optuna version: {}\". format(optuna.__version__))\n",
    "\n",
    "import mlflow\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_PARENT_RUN_ID\n",
    "from mlflow.tracking import MlflowClient\n",
    "print(\"mlflow version: {}\". format(mlflow.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "import yaml\n",
    "with open('ml_parameter.yaml') as file:\n",
    "  config_data= yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 3.4\n",
    "SCRIPT = \"titanic_BasicModels_pipeline\"\n",
    "\n",
    "# define cross validation\n",
    "cv = StratifiedShuffleSplit(\n",
    "    n_splits = config_data[\"N_SPLITS\"],\n",
    "    test_size = config_data[\"TEST_SIZE\"],\n",
    "    random_state = config_data[\"RANDOM_STATE\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "try:\n",
    "    experiment = client.create_experiment(config_data[\"experiment_name\"])\n",
    "except:\n",
    "    experiment = client.get_experiment_by_name(config_data[\"experiment_name\"]).experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df_train = pd.read_pickle('../03_DataPreprocessing/df_train_prepared_unfinished.pkl')\n",
    "    df_test = pd.read_pickle('../03_DataPreprocessing/df_test_prepared_unfinished.pkl')\n",
    "\n",
    "    df_train.drop(['PassengerId', 'Name', 'dataset', 'First', 'Last', 'TGroup'], axis=1, inplace=True)\n",
    "    df_test.drop(['PassengerId', 'Name',  'dataset', 'Survived', 'First', 'Last', 'TGroup'], axis=1, inplace=True)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitter:\n",
    "    \"\"\"The class is used to split the data\"\"\"\n",
    "\n",
    "    def __init__(self, test_size:float):\n",
    "        self.test_size = test_size\n",
    "\n",
    "        self.X_train: pd.DataFrame = None\n",
    "        self.y_train: pd.DataFrame = None\n",
    "        self.X_validate: pd.DataFrame = None\n",
    "        self.y_validate: pd.DataFrame = None\n",
    "\n",
    "\n",
    "    def split(self, df_train: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Split the data between train and test.\"\"\"\n",
    "        y_train = df_train['Survived']\n",
    "        x_train = df_train.drop(['Survived'], axis=1)\n",
    "\n",
    "        self.X_train, self.X_validate, self.y_train, self.y_validate = train_test_split(x_train, y_train, test_size=self.test_size)\n",
    "\n",
    "        return self.X_train, self.y_train, self.X_validate, self.y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splitter = DataSplitter(test_size=config_data[\"TEST_SIZE\"])\n",
    "x_train, y_train, x_validate, y_validate = data_splitter.split(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiningTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, n_bins_fare:int, n_bins_age:int):\n",
    "        self.n_bins_fare = n_bins_fare\n",
    "        self.n_bins_age = n_bins_age\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, y:Optional[pd.DataFrame]=None) -> \"BiningTransformer\":\n",
    "        self.x_train = x.copy()\n",
    "        return self\n",
    "\n",
    "    def transform(self, x:pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        x['Fare_bin'] = pd.qcut(x['Fare'], self.n_bins_fare, labels=False)\n",
    "        x['Age_bin'] = pd.qcut(x['Age'], self.n_bins_age, labels=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkewedFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, transform_skewed_features_flag:bool):\n",
    "        self.transform_skewed_features_flag = transform_skewed_features_flag\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, y:Optional[pd.DataFrame]=None) -> \"BiningTransformer\":\n",
    "        self.x_train = x.copy()\n",
    "        return self\n",
    "\n",
    "    def transform(self, x:pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.transform_skewed_features_flag==True:\n",
    "            x[\"Fare\"] = x[\"Fare\"].apply(np.log)\n",
    "             # the not transformed data that contains 0\n",
    "            # after the transformation we have -inf values that have to be replaced by 0\n",
    "            x[\"Fare\"][np.isneginf(x[\"Fare\"])]=0\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, ohe_min_frequency:float, ohe_max_categories:float):\n",
    "        self.ohe_min_frequency = ohe_min_frequency\n",
    "        self.ohe_max_categories = ohe_max_categories\n",
    "        self.cat_vars=None\n",
    "        self.enc=None\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, y:Optional[pd.DataFrame]=None) -> \"OneHotEncoderTransformer\":\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False, drop=\"if_binary\",\n",
    "            min_frequency=self.ohe_min_frequency, max_categories=self.ohe_max_categories)\n",
    "\n",
    "        self.cat_vars = x.dtypes[x.dtypes == \"object\"].index\n",
    "        self.enc = enc.fit(x[self.cat_vars])\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, x:pd.DataFrame) -> pd.DataFrame:\n",
    "        ohe = pd.DataFrame(self.enc.transform(x[self.cat_vars]), columns=self.enc.get_feature_names())\n",
    "        \n",
    "        x.reset_index(drop=True, inplace=True)\n",
    "        ohe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        x = pd.concat([x, ohe], axis=1).drop(self.cat_vars, axis=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowVarianceTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, variance_threshold:float):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.sel_features=None\n",
    "        self.sel=None\n",
    "        self.sel_features_reduced=None\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, y:Optional[pd.DataFrame]=None) -> \"LowVarianceTransformer\":\n",
    "        # remove all features that are either one or zero in more than 95% of the samples\n",
    "        sel = VarianceThreshold(threshold=(self.variance_threshold * (1 - self.variance_threshold)))\n",
    "        self.sel_features = list(x)\n",
    "        # fit the VarianceThreshold object to the training data\n",
    "        self.sel = sel.fit(x[self.sel_features])\n",
    "\n",
    "        # get the column names after the variance threshold reduction\n",
    "        self.sel_features_reduced = [self.sel_features[i] for i in self.sel.get_support(indices=True)]\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, x:pd.DataFrame) -> pd.DataFrame:\n",
    "        x = pd.DataFrame(self.sel.transform(x[self.sel_features]), columns=self.sel_features_reduced)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, correlation_threshold:float):\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.to_drop=None\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, y:Optional[pd.DataFrame]=None) -> \"CorrelationTransformer\":\n",
    "        corr_matrix = x.corr().abs()\n",
    "\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        # Find features with correlation higher than 0.9 or lower -0.9\n",
    "        self.to_drop = [column for column in upper.columns if any((upper[column] > self.correlation_threshold) | (upper[column] < -self.correlation_threshold))]\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, x:pd.DataFrame) -> pd.DataFrame:\n",
    "        x =  x.drop(self.to_drop, axis=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columnprep__transformers_num) -> None:\n",
    "        self.columnprep__transformers_num = columnprep__transformers_num\n",
    "        self.transformer_not_num=None\n",
    "        self.transformer_num=None\n",
    "\n",
    "        if columnprep__transformers_num == \"StandardScaler\":\n",
    "            self.scaler = StandardScaler()\n",
    "        elif columnprep__transformers_num == \"MinMaxScaler\":\n",
    "            self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, y:Optional[pd.DataFrame]=None) -> \"ScalerTransformer\":\n",
    "        self.transformer_not_num = [col for col in list(x) if (col.startswith(\"x\") & col[1].isnumeric())]\n",
    "        self.transformer_num = [col for col in list(x) if col not in self.transformer_not_num]\n",
    "        \n",
    "        self.scaler.fit(x[self.transformer_num], y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, x:pd.DataFrame) -> pd.DataFrame:\n",
    "        x_transform = self.scaler.transform(x[self.transformer_num])\n",
    "        x_transform = pd.DataFrame(x_transform, index=x.index, columns=x[self.transformer_num].columns)\n",
    "        \n",
    "        return pd.concat([x_transform, x[self.transformer_not_num]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial, model_type, child_run):\n",
    "\n",
    "    # create parameter for data preprocessing pipeline\n",
    "    n_bins_fare=trial.suggest_int('preprocessing_n_bins_fare', 5, 15)\n",
    "    n_bins_age=trial.suggest_int('preprocessing_n_bins_age', 5, 15)\n",
    "    transform_skewed_features_flag=trial.suggest_categorical(\"preprocessing_transform_skewed_features_flag\", [True, False])\n",
    "    ohe_min_frequency=trial.suggest_float(\"preprocessing_ohe_min_frequency\", 0, 0.2, log=False)\n",
    "    ohe_max_categories=trial.suggest_int('preprocessing_ohe_max_categories', 5, 20)\n",
    "    feature_selection_low_variance_flag=trial.suggest_categorical(\"preprocessing_feature_selection_low_variance_flag\", [True, False])\n",
    "    correlation=trial.suggest_float(\"preprocessing_correlation\", 0.7, 0.95, log=False)\n",
    "\n",
    "    # log all parameters of the data preprocessing with mlflow\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_n_bins_fare\", n_bins_fare)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_n_bins_age\", n_bins_age)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_transform_skewed_features_flag\", transform_skewed_features_flag)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_ohe_min_frequency\", ohe_min_frequency)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_ohe_max_categories\", ohe_max_categories)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_feature_selection_low_variance_flag\", feature_selection_low_variance_flag)\n",
    "    client.log_param(child_run.info.run_id, \"preprocessing_correlation\", correlation)  \n",
    "\n",
    "\n",
    "    ''' columnprep '''\n",
    "    columnprep__transformers_num = trial.suggest_categorical(\"columnprep__transformers_num\", [\"StandardScaler\", \"MinMaxScaler\"])\n",
    "\n",
    "\n",
    "    ''' algo '''\n",
    "    if model_type == 'SVC':\n",
    "        svm_kernel = trial.suggest_categorical('svm_kernel', config_data[\"svm_kernel\"])\n",
    "        svm_C = trial.suggest_float('svm_C', config_data[\"svm_C\"][0], config_data[\"svm_C\"][1], log=True)\n",
    "        svm_degree = trial.suggest_discrete_uniform('svm_degree', config_data[\"svm_degree\"][0], config_data[\"svm_degree\"][1], config_data[\"svm_degree\"][2])\n",
    "        \n",
    "        model = SVC(\n",
    "            kernel=svm_kernel,\n",
    "            C=svm_C,\n",
    "            degree=svm_degree,\n",
    "            probability=True,\n",
    "            random_state=config_data[\"RANDOM_STATE\"]\n",
    "        )\n",
    "\n",
    "        client.log_param(child_run.info.run_id, \"svm_kernel\", svm_kernel)\n",
    "        client.log_param(child_run.info.run_id, \"svm_C\", svm_C)\n",
    "        client.log_param(child_run.info.run_id, \"svm_degree\", svm_degree)\n",
    "    \n",
    "\n",
    "    if model_type == 'LogisticRegression':\n",
    "        lr_C = trial.suggest_float(\"lr_C\", config_data[\"lr_C\"][0], config_data[\"lr_C\"][1], log=True)\n",
    "        lr_penalty = trial.suggest_categorical('lr_penalty', config_data[\"lr_penalty\"])\n",
    "        if lr_penalty == 'l1':\n",
    "            lr_solver = 'saga'\n",
    "        else:\n",
    "            lr_solver = 'lbfgs'\n",
    "        \n",
    "        model = LogisticRegression(\n",
    "            C=lr_C,\n",
    "            penalty=lr_penalty,\n",
    "            solver=lr_solver,\n",
    "            random_state=config_data[\"RANDOM_STATE\"],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        client.log_param(child_run.info.run_id, \"lr_C\", lr_C)\n",
    "        client.log_param(child_run.info.run_id, \"lr_penalty\", lr_penalty)\n",
    "        client.log_param(child_run.info.run_id, \"lr_solver\", lr_solver)\n",
    "\n",
    "\n",
    "    if model_type == 'DecisionTreeClassifier':\n",
    "        dt_max_depth = trial.suggest_int('dt_max_depth', config_data[\"dt_max_depth\"][0], x_train.shape[1])\n",
    "        dt_criterion = trial.suggest_categorical(\"dt_criterion\", config_data[\"dt_criterion\"])\n",
    "        dt_max_leaf_nodes = trial.suggest_int(\"dt_max_leaf_nodes\", config_data[\"dt_max_leaf_nodes\"][0], config_data[\"dt_max_leaf_nodes\"][1])\n",
    "        \n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=dt_max_depth,\n",
    "            criterion=dt_criterion,\n",
    "            max_leaf_nodes=dt_max_leaf_nodes,\n",
    "            random_state=config_data[\"RANDOM_STATE\"]\n",
    "          )\n",
    "    \n",
    "        client.log_param(child_run.info.run_id, \"dt_max_depth\", dt_max_depth)\n",
    "        client.log_param(child_run.info.run_id, \"dt_criterion\", dt_criterion)\n",
    "        client.log_param(child_run.info.run_id, \"dt_max_leaf_nodes\", dt_max_leaf_nodes)\n",
    "    \n",
    "\n",
    "\n",
    "    if model_type == 'ExtraTreesClassifier':\n",
    "        etc_n_estimators = trial.suggest_int('etc_n_estimators', config_data[\"etc_n_estimators\"][0], config_data[\"etc_n_estimators\"][1])\n",
    "        etc_max_depth = trial.suggest_int('etc_max_depth', config_data[\"etc_max_depth\"][0], x_train.shape[1])\n",
    "        etc_min_samples_split = trial.suggest_float('etc_min_samples_split', config_data[\"etc_min_samples_split\"][0], config_data[\"etc_min_samples_split\"][1])\n",
    "        etc_min_samples_leaf = trial.suggest_float('etc_min_samples_leaf', config_data[\"etc_min_samples_leaf\"][0], config_data[\"etc_min_samples_leaf\"][1])\n",
    "        etc_criterion = trial.suggest_categorical(\"etc_criterion\", config_data[\"etc_criterion\"])\n",
    "        etc_max_leaf_nodes = trial.suggest_int(\"etc_max_leaf_nodes\", config_data[\"etc_max_leaf_nodes\"][0], config_data[\"etc_max_leaf_nodes\"][1])\n",
    "        \n",
    "        model = ExtraTreesClassifier(\n",
    "            n_estimators=etc_n_estimators,\n",
    "            max_depth=etc_max_depth,\n",
    "            min_samples_split=etc_min_samples_split,\n",
    "            min_samples_leaf=etc_min_samples_leaf,\n",
    "            criterion=etc_criterion,\n",
    "            max_leaf_nodes=etc_max_leaf_nodes,\n",
    "            random_state=config_data[\"RANDOM_STATE\"],\n",
    "            n_jobs=-1\n",
    "          )\n",
    "    \n",
    "        client.log_param(child_run.info.run_id, \"etc_n_estimators\", etc_n_estimators)\n",
    "        client.log_param(child_run.info.run_id, \"etc_max_depth\", etc_max_depth)\n",
    "        client.log_param(child_run.info.run_id, \"etc_min_samples_split\", etc_min_samples_split)\n",
    "        client.log_param(child_run.info.run_id, \"etc_min_samples_leaf\", etc_min_samples_leaf)\n",
    "        client.log_param(child_run.info.run_id, \"etc_criterion\", etc_criterion)\n",
    "        client.log_param(child_run.info.run_id, \"etc_max_leaf_nodes\", etc_max_leaf_nodes)\n",
    "\n",
    "\n",
    "    if model_type == 'RandomForestClassifier':\n",
    "        rfc_n_estimators = trial.suggest_int('rfc_n_estimators', config_data[\"rfc_n_estimators\"][0], config_data[\"rfc_n_estimators\"][1])\n",
    "        rfc_max_depth = trial.suggest_int('rfc_max_depth', config_data[\"rfc_max_depth\"][0], x_train.shape[1])\n",
    "        rfc_min_samples_split = trial.suggest_float('rfc_min_samples_split', config_data[\"rfc_min_samples_split\"][0], config_data[\"rfc_min_samples_split\"][1])\n",
    "        rfc_min_samples_leaf = trial.suggest_float('rfc_min_samples_leaf', config_data[\"rfc_min_samples_leaf\"][0], config_data[\"rfc_min_samples_leaf\"][1])\n",
    "        rfc_criterion = trial.suggest_categorical(\"rfc_criterion\", config_data[\"rfc_criterion\"])\n",
    "        rfc_max_leaf_nodes = trial.suggest_int(\"rfc_max_leaf_nodes\", config_data[\"rfc_max_leaf_nodes\"][0], config_data[\"rfc_max_leaf_nodes\"][1])\n",
    "        \n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=rfc_n_estimators,\n",
    "            max_depth=rfc_max_depth,\n",
    "            min_samples_split=rfc_min_samples_split,\n",
    "            min_samples_leaf=rfc_min_samples_leaf,\n",
    "            criterion=rfc_criterion,\n",
    "            max_leaf_nodes=rfc_max_leaf_nodes,\n",
    "            random_state=config_data[\"RANDOM_STATE\"],\n",
    "            n_jobs=-1\n",
    "          )\n",
    "    \n",
    "        client.log_param(child_run.info.run_id, \"rfc_n_estimators\", rfc_n_estimators)\n",
    "        client.log_param(child_run.info.run_id, \"rfc_max_depth\", rfc_max_depth)\n",
    "        client.log_param(child_run.info.run_id, \"rfc_min_samples_split\", rfc_min_samples_split)\n",
    "        client.log_param(child_run.info.run_id, \"rfc_min_samples_leaf\", rfc_min_samples_leaf)\n",
    "        client.log_param(child_run.info.run_id, \"rfc_criterion\", rfc_criterion)\n",
    "        client.log_param(child_run.info.run_id, \"rfc_max_leaf_nodes\", rfc_max_leaf_nodes)\n",
    "\n",
    "\n",
    "    if model_type == 'XGBClassifier':\n",
    "        xgb_n_estimators = trial.suggest_int('xgb_n_estimators', config_data[\"xgb_n_estimators\"][0], config_data[\"xgb_n_estimators\"][1])\n",
    "        xgb_learning_rate = trial.suggest_float(\"xgb_learning_rate\", config_data[\"xgb_learning_rate\"][0], config_data[\"xgb_learning_rate\"][1], log=True)\n",
    "        xgb_reg_lambda = trial.suggest_float(\"xgb_reg_lambda\", config_data[\"xgb_reg_lambda\"][0], config_data[\"xgb_reg_lambda\"][1], log=True)\n",
    "        xgb_reg_alpha = trial.suggest_float(\"xgb_reg_alpha\", config_data[\"xgb_reg_alpha\"][0], config_data[\"xgb_reg_alpha\"][1], log=True)\n",
    "        \n",
    "        model = XGBClassifier(\n",
    "            n_estimators=xgb_n_estimators,\n",
    "            learning_rate=xgb_learning_rate,\n",
    "            reg_lambda=xgb_reg_lambda,\n",
    "            reg_alpha=xgb_reg_alpha,\n",
    "            random_state=config_data[\"RANDOM_STATE\"],\n",
    "            n_jobs = -1\n",
    "          )\n",
    "    \n",
    "        client.log_param(child_run.info.run_id, \"xgb_n_estimators\", xgb_n_estimators)\n",
    "        client.log_param(child_run.info.run_id, \"xgb_learning_rate\", xgb_learning_rate)\n",
    "        client.log_param(child_run.info.run_id, \"xgb_reg_lambda\", xgb_reg_lambda)\n",
    "        client.log_param(child_run.info.run_id, \"xgb_reg_alpha\", xgb_reg_alpha)\n",
    "\n",
    "\n",
    "    if model_type == 'LGBMClassifier':\n",
    "        lgb_n_estimators = trial.suggest_int('lgb_n_estimators', config_data[\"lgb_n_estimators\"][0], config_data[\"lgb_n_estimators\"][1])\n",
    "        lgb_learning_rate = trial.suggest_float(\"lgb_learning_rate\", config_data[\"lgb_learning_rate\"][0], config_data[\"lgb_learning_rate\"][1], log=True)\n",
    "        lgb_max_depth = trial.suggest_int('lgb_max_depth', config_data[\"lgb_max_depth\"][0], config_data[\"lgb_max_depth\"][1])\n",
    "        lgb_num_leaves = trial.suggest_int('lgb_num_leaves', config_data[\"lgb_num_leaves\"][0], 2^lgb_max_depth+3)\n",
    "        lgb_min_data_in_leaf = trial.suggest_int('lgb_min_data_in_leaf', config_data[\"lgb_min_data_in_leaf\"][0], config_data[\"lgb_min_data_in_leaf\"][1])\n",
    "        lgb_subsample = trial.suggest_float('lgb_subsample', config_data[\"lgb_subsample\"][0], config_data[\"lgb_subsample\"][1])\n",
    "        lgb_feature_fraction = trial.suggest_float('lgb_feature_fraction', config_data[\"lgb_feature_fraction\"][0], config_data[\"lgb_feature_fraction\"][1])\n",
    "        lgb_reg_lambda = trial.suggest_float(\"lgb_reg_lambda\", config_data[\"lgb_reg_lambda\"][0], config_data[\"lgb_reg_lambda\"][1], log=True)\n",
    "        lgb_reg_alpha = trial.suggest_float(\"lgb_reg_alpha\", config_data[\"lgb_reg_alpha\"][0], config_data[\"lgb_reg_alpha\"][1], log=True)\n",
    "        \n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=lgb_n_estimators,\n",
    "            learning_rate=lgb_learning_rate,\n",
    "            max_depth=lgb_max_depth,\n",
    "            num_leaves=lgb_num_leaves,\n",
    "            min_data_in_leaf=lgb_min_data_in_leaf,\n",
    "            subsample=lgb_subsample,\n",
    "            feature_fraction=lgb_feature_fraction,\n",
    "            reg_lambda=lgb_reg_lambda,\n",
    "            reg_alpha=lgb_reg_alpha,\n",
    "            # num_threads = 4,\n",
    "            random_state=config_data[\"RANDOM_STATE\"],\n",
    "            n_jobs=-1\n",
    "            # device_type='cuda_exp'\n",
    "          )\n",
    "    \n",
    "        client.log_param(child_run.info.run_id, \"lgb_n_estimators\", lgb_n_estimators)\n",
    "        client.log_param(child_run.info.run_id, \"lgb_learning_rate\", lgb_learning_rate)\n",
    "        client.log_param(child_run.info.run_id, \"lgb_max_depth\", lgb_max_depth)\n",
    "        client.log_param(child_run.info.run_id, \"lgb_num_leaves\", lgb_num_leaves)\n",
    "        client.log_param(child_run.info.run_id, \"lgb_min_data_in_leaf\", lgb_min_data_in_leaf)\n",
    "        client.log_param(child_run.info.run_id, \"lgb_subsample\", lgb_subsample)\n",
    "        client.log_param(child_run.info.run_id, \"lgb_feature_fraction\", lgb_feature_fraction)\n",
    "        client.log_param(child_run.info.run_id, \"lgb_reg_lambda\", lgb_reg_lambda)\n",
    "        client.log_param(child_run.info.run_id, \"lgb_reg_alpha\", lgb_reg_alpha)\n",
    "\n",
    "\n",
    "    if model_type == 'GradientBoostingClassifier':\n",
    "        gbc_n_estimators = trial.suggest_int('gbc_n_estimators', config_data[\"gbc_n_estimators\"][0], config_data[\"gbc_n_estimators\"][1])\n",
    "        gbc_learning_rate = trial.suggest_float(\"gbc_learning_rate\", config_data[\"gbc_learning_rate\"][0], config_data[\"gbc_learning_rate\"][1], log=True)\n",
    "        gbc_max_depth = trial.suggest_int('gbc_max_depth', config_data[\"gbc_max_depth\"][0], config_data[\"gbc_max_depth\"][1])\n",
    "        gbc_min_samples_split = trial.suggest_float('gbc_min_samples_split', config_data[\"gbc_min_samples_split\"][0], config_data[\"gbc_min_samples_split\"][1])\n",
    "        gbc_min_samples_leaf = trial.suggest_float('gbc_min_samples_leaf', config_data[\"gbc_min_samples_leaf\"][0], config_data[\"gbc_min_samples_leaf\"][1])\n",
    "        gbc_max_leaf_nodes = trial.suggest_int(\"gbc_max_leaf_nodes\", config_data[\"gbc_max_leaf_nodes\"][0], config_data[\"gbc_max_leaf_nodes\"][1])\n",
    "        \n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=gbc_n_estimators,\n",
    "            learning_rate=gbc_learning_rate,\n",
    "            max_depth=gbc_max_depth,\n",
    "            min_samples_split=gbc_min_samples_split,\n",
    "            min_samples_leaf=gbc_min_samples_leaf,\n",
    "            max_leaf_nodes=gbc_max_leaf_nodes,\n",
    "            random_state=config_data[\"RANDOM_STATE\"],\n",
    "          )\n",
    "    \n",
    "        client.log_param(child_run.info.run_id, \"gbc_n_estimators\", gbc_n_estimators)\n",
    "        client.log_param(child_run.info.run_id, \"gbc_learning_rate\", gbc_learning_rate)\n",
    "        client.log_param(child_run.info.run_id, \"gbc_max_depth\", gbc_max_depth)\n",
    "        client.log_param(child_run.info.run_id, \"gbc_min_samples_split\", gbc_min_samples_split)\n",
    "        client.log_param(child_run.info.run_id, \"gbc_min_samples_leaf\", gbc_min_samples_leaf)\n",
    "        client.log_param(child_run.info.run_id, \"gbc_max_leaf_nodes\", gbc_max_leaf_nodes)\n",
    "\n",
    "\n",
    "\n",
    "    client.log_param(child_run.info.run_id, \"algo\", model.__class__.__name__)\n",
    "\n",
    "\n",
    "    pipeline = make_pipeline(\n",
    "        BiningTransformer(n_bins_fare=n_bins_fare, n_bins_age=n_bins_age),\n",
    "        SkewedFeatureTransformer(transform_skewed_features_flag=transform_skewed_features_flag),\n",
    "        OneHotEncoderTransformer(ohe_min_frequency=ohe_min_frequency, ohe_max_categories=ohe_max_categories),\n",
    "        LowVarianceTransformer(variance_threshold=0.95),\n",
    "        CorrelationTransformer(correlation_threshold=correlation),\n",
    "        ScalerTransformer(columnprep__transformers_num=columnprep__transformers_num),\n",
    "        model,\n",
    "        verbose=True\n",
    "    )\n",
    "            \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_train, y_train, y_validate, y_validate_pred, y_validate_scores, pipeline, child_run):\n",
    "\n",
    "    \"\"\"\n",
    "    evaluate the classification model with\n",
    "    - classification report\n",
    "    - precision-recall-curve\n",
    "    - ROC curve\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_learning_curve(pipeline, x_train, y_train):\n",
    "\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            pipeline,\n",
    "            x_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            train_sizes=np.linspace(.1, 1.0, 8)\n",
    "            )\n",
    "\n",
    "\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel(\"Training examples\")\n",
    "        ax1.set_ylabel(\"Score\")\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        ax1.grid()\n",
    "\n",
    "        ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                        train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                        color=\"r\")\n",
    "        ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                        test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                label=\"Training score\")\n",
    "        ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                label=\"Cross-validation score\")\n",
    "\n",
    "        ax1.legend(loc=\"best\")\n",
    "        ax1.set_title(\"Difference between training and CV: \"\\\n",
    "            + str(round(test_scores_mean[7] / train_scores_mean[7] * 100, 2))\\\n",
    "            + \"%\")\n",
    "        client.log_figure(child_run.info.run_id, fig1, 'plot_learning_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def plot_confusion_matrix(y_validate, y_validate_pred):\n",
    "        group_names = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n",
    "        group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                        confusion_matrix(y_validate, y_validate_pred).flatten()]\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                             confusion_matrix(y_validate, y_validate_pred).flatten()/np.sum(confusion_matrix(y_validate, y_validate_pred))]\n",
    "        labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "                  zip(group_names,group_counts,group_percentages)]\n",
    "        labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "        fig2, ax2 = plt.subplots()\n",
    "        sns.heatmap(confusion_matrix(y_validate, y_validate_pred), annot=labels, fmt=\"\", cmap='Blues')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        client.log_figure(child_run.info.run_id, fig2, 'plot_confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def plot_precision_recall_vs_threshold(y_validate, y_scores, child_run):\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_validate, y_scores)\n",
    "\n",
    "        # convert to f score\n",
    "        fscore = (2 * precisions * recalls) / (precisions + recalls)\n",
    "        # locate the index of the largest f score\n",
    "        ix = np.argmax(fscore)\n",
    "        \n",
    "        client.log_metric(child_run.info.run_id, \"f1_score\", round(fscore[ix], 5))\n",
    "\n",
    "        fig3, ax3 = plt.subplots()\n",
    "        ax3.plot(thresholds, precisions[:-1], \"b\", label=\"Precision\")\n",
    "        ax3.plot(thresholds, recalls[:-1], \"g\", label=\"Recall\")\n",
    "        ax3.plot(thresholds, fscore[:-1], \"r\", label=\"F1 Score\")\n",
    "        ax3.axvline(x=thresholds[ix], color='red', linestyle='--')\n",
    "        plt.axhline(y=precisions[ix], color='b', linestyle='--')\n",
    "        plt.axhline(y=recalls[ix], color='g', linestyle='--')\n",
    "        ax3.set_xlabel(\"Threshold\")\n",
    "        ax3.legend(loc=\"upper left\")\n",
    "        ax3.set_ylim([0,1])\n",
    "        client.log_figure(child_run.info.run_id, fig3, 'plot_f1.png')\n",
    "        plt.close()\n",
    "\n",
    "        fig4, ax4 = plt.subplots()\n",
    "        ax4.plot(recalls, precisions, marker='.', label='Logistic')\n",
    "        ax4.scatter(recalls[ix], precisions[ix], 200, marker='o', color='red', label='Best')\n",
    "        ax4.set_xlabel('Recall')\n",
    "        ax4.set_ylabel('Precision')\n",
    "        client.log_figure(child_run.info.run_id, fig4, 'plot_precision_recall.png')\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "    def plot_roc_curve(y_validate, y_scores, child_run):\n",
    "        fpr, tpr, thresholds = roc_curve(y_validate, y_scores)\n",
    "\n",
    "        roc_auc = round(roc_auc_score(y_validate, y_scores), 3)\n",
    "        \n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "\n",
    "        fig5, ax5 = plt.subplots()\n",
    "        ax5.plot(fpr, tpr, linewidth=2)\n",
    "        ax5.plot([0,1], [0,1], 'k--')\n",
    "        ax5.axis([0,1,0,1])\n",
    "        ax5.scatter(fpr[optimal_idx], tpr[optimal_idx], 200, marker='o', color='red', label='Best')\n",
    "        ax5.set_xlabel('False Positive Rate')\n",
    "        ax5.set_ylabel('True Positive Rate')\n",
    "        client.log_figure(child_run.info.run_id, fig5, 'plot_roc_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        client.log_metric(child_run.info.run_id, \"roc_auc\", roc_auc)\n",
    "\n",
    "        \n",
    "\n",
    "    plot_confusion_matrix(y_validate, y_validate_pred)\n",
    "    plot_precision_recall_vs_threshold(y_validate, y_validate_scores, child_run)\n",
    "    plot_roc_curve(y_validate, y_validate_scores, child_run)\n",
    "    plot_learning_curve(pipeline, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(best_model, x_test, parent_run):\n",
    "    # predict the test values with the training classification model\n",
    "    y_pred = best_model.predict(x_test).astype(int)\n",
    "    \n",
    "    df_submission = pd.read_csv(\"../01_RawData/gender_submission.csv\")\n",
    "    df_submission['Survived'] = y_pred\n",
    "    \n",
    "    df_submission.to_csv('submissions/%s.csv'%parent_run.info.run_id, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    \n",
    "    def __init__(self, model_type, parent_run):\n",
    "        self.best_model = None\n",
    "        self._model = None\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.parent_run = parent_run\n",
    "\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "        start_time = time.time()\n",
    "\n",
    "        child_run = client.create_run(\n",
    "            experiment_id=experiment,\n",
    "            tags={\n",
    "                MLFLOW_PARENT_RUN_ID: self.parent_run.info.run_id,\n",
    "                \"mlflow.runName\": SCRIPT,\n",
    "                \"script_version\": VERSION\n",
    "            }\n",
    "        )\n",
    "\n",
    "        \"\"\" Machine Learning Model Creation \"\"\"\n",
    "        pipeline = create_model(trial, self.model_type, child_run)\n",
    "        self._model = pipeline\n",
    "\n",
    "        \"\"\" Train and Cross Validation of Machine Learning Model \"\"\"\n",
    "        score = cross_val_score(\n",
    "            pipeline,\n",
    "            x_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=\"accuracy\",\n",
    "            n_jobs=-1\n",
    "        ).mean()\n",
    "        client.log_metric(child_run.info.run_id, \"cv_score\", score)\n",
    "\n",
    "        client.log_metric(child_run.info.run_id, \"runtime\", time.time() - start_time)\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "    def callback(self, study, trial):\n",
    "        if study.best_trial == trial:\n",
    "            self.best_model = self._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_mlrun(model_type):\n",
    "\n",
    "  parent_run = client.create_run(experiment_id=experiment, tags={\"mlflow.runName\": SCRIPT, \"script_version\": VERSION})\n",
    "\n",
    "  objective = Objective(model_type, parent_run)\n",
    "\n",
    "  study = optuna.create_study(\n",
    "    sampler = optuna.samplers.TPESampler(),\n",
    "    direction=\"maximize\"\n",
    "    )\n",
    "\n",
    "  study.optimize(\n",
    "    objective,\n",
    "    n_trials=config_data[\"N_TRAILS\"],\n",
    "    timeout=config_data[\"TIMEOUT\"],\n",
    "    n_jobs=-1,\n",
    "    callbacks=[objective.callback]\n",
    "    )\n",
    "\n",
    "  print(\"Best trial:\")\n",
    "  print(study.best_value)\n",
    "  print(study.best_params)\n",
    "\n",
    "  client.log_metric(parent_run.info.run_id, \"best_cv_score\", round(study.best_value, 3))\n",
    "  \n",
    "  client.log_param(parent_run.info.run_id, \"cv_n_splits\", cv.n_splits)\n",
    "  client.log_param(parent_run.info.run_id, \"cv_train_size\", cv.train_size)\n",
    "  client.log_param(parent_run.info.run_id, \"cv_test_size\", cv.test_size)\n",
    "  client.log_param(parent_run.info.run_id, \"cv_random_state\", cv.random_state)\n",
    "\n",
    "  print(\"Log best parameters\")\n",
    "  for param in study.best_params:\n",
    "    client.log_param(parent_run.info.run_id, param, study.best_params[param])\n",
    "\n",
    "\n",
    "  print(\"Save best model\")\n",
    "  # save the best model as file\n",
    "  best_model = objective.best_model\n",
    "  mlflow.sklearn.save_model(best_model, \"models/%s/\"%parent_run.info.run_id)\n",
    "  client.log_param(parent_run.info.run_id, \"algo\", best_model.get_params()[\"steps\"][-1][1].__class__.__name__)\n",
    "\n",
    "  print(\"Fit best model\")\n",
    "  # fit the pipeline with the total dataset to compute the validation results\n",
    "  best_model.fit(df_train.drop(['Survived'], axis=1), df_train['Survived'])\n",
    "\n",
    "  print(\"Create submission\")\n",
    "  # create submission of best model\n",
    "  create_submission(best_model, df_test, parent_run)\n",
    "\n",
    "  print(\"Predict training outcome\")\n",
    "  # predict the training outcome\n",
    "  y_validate_pred = best_model.predict(x_validate)\n",
    "\n",
    "  # predict probabilities\n",
    "  y_validate_proba = best_model.predict_proba(x_validate)\n",
    "  # keep probabilities for the positive outcome only\n",
    "  y_validate_scores = y_validate_proba[:, 1]\n",
    "\n",
    "  print(\"Evaluate model performance\")\n",
    "  evaluate_model(x_train, y_train, y_validate, y_validate_pred, y_validate_scores, best_model, parent_run)\n",
    "\n",
    "  mlflow.end_run()\n",
    "\n",
    "  return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_lr = create_new_mlrun(model_type='LogisticRegression')\n",
    "# optuna.visualization.plot_optimization_history(study_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_dt = create_new_mlrun(model_type='DecisionTreeClassifier')\n",
    "# optuna.visualization.plot_optimization_history(study_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_svm = create_new_mlrun(model_type='SVC')\n",
    "# optuna.visualization.plot_optimization_history(study_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_svm = create_new_mlrun(model_type='ExtraTreesClassifier')\n",
    "# optuna.visualization.plot_optimization_history(study_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_svm = create_new_mlrun(model_type='GradientBoostingClassifier')\n",
    "# optuna.visualization.plot_optimization_history(study_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_svm = create_new_mlrun(model_type='RandomForestClassifier')\n",
    "# optuna.visualization.plot_optimization_history(study_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Kaggle_score(run_id, kaggle_score):\n",
    "    # show if kaggle_score is already present\n",
    "    if \"kaggle_score\" not in dict(dict(mlflow.get_run(run_id))[\"data\"])[\"metrics\"].keys():\n",
    "        # if no kaggle_score is present, start run and write kaggle_score\n",
    "        with mlflow.start_run(run_id=run_id):\n",
    "            mlflow.log_metric(\"kaggle_score\", kaggle_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_Kaggle_score(run_id=\"3d825586491e4200a6e42cc971402487\", kaggle_score=0.76315) # GradientBoostingClassifier\n",
    "add_Kaggle_score(run_id=\"37ba72127e6d4572bf74163456c00d91\", kaggle_score=0.78708) # SVC\n",
    "add_Kaggle_score(run_id=\"45706d2298f841549a5606afc8387d1c\", kaggle_score=0.78229) # LogisticRegression\n",
    "add_Kaggle_score(run_id=\"0b378cec02af4ce6b3acd22bc160af58\", kaggle_score=0.77751) # DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlflow_optuna')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4aa63c7bcea9117a32328ad03333d01dc516bdcdb33b6eb92ab7a393341400f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
