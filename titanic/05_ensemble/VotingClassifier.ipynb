{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "# numpy: support for large, multi-dimensional arrays and matrices and high-level mathematical functions\n",
    "import numpy as np\n",
    "print(\"numpy version: {}\". format(np.__version__))\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit, train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, confusion_matrix, roc_auc_score\n",
    "print(\"sklearn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "print(\"xgboost version: {}\". format(xgboost.__version__))\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "print(\"lightgbm version: {}\". format(lightgbm.__version__))\n",
    "\n",
    "import optuna\n",
    "print(\"optuna version: {}\". format(optuna.__version__))\n",
    "\n",
    "import mlflow\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_PARENT_RUN_ID\n",
    "from mlflow.tracking import MlflowClient\n",
    "print(\"mlflow version: {}\". format(mlflow.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Ensemble models to improve the accuracy of the model\n",
    "\n",
    "## Used ensembling techniques\n",
    "- AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # load prepared training and test dataset\n",
    "    df_train = pd.read_pickle('../03_dataCleaningPreparation/df_train_prepared.pkl')\n",
    "    df_test = pd.read_pickle('../03_dataCleaningPreparation/df_test_prepared.pkl')\n",
    "\n",
    "    # split the training and test dataset to the input features (x_train, x_test) and the survival class (y_train)\n",
    "    y_train = df_train['Survived']\n",
    "    x_train = df_train.drop(['Survived'], axis=1)\n",
    "    x_test = df_test\n",
    "\n",
    "    x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.3, stratify = y_train, random_state = 42)\n",
    "\n",
    "    return x_train, y_train, x_validate, y_validate, x_test\n",
    "\n",
    "x_train, y_train, x_validate, y_validate, x_test = load_data()\n",
    "\n",
    "\n",
    "\n",
    "def create_submission(best_model, x_test, name):\n",
    "    # predict the test values with the training classification model\n",
    "    y_pred = best_model.predict(x_test).astype(int)\n",
    "    \n",
    "    df_submission = pd.read_csv(\"../01_rawdata/gender_submission.csv\")\n",
    "    df_submission['Survived'] = y_pred\n",
    "  \n",
    "    df_submission.to_csv('submissions/%s.csv'%name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all results from the experiment\n",
    "df = mlflow.search_runs(experiment_names=[\"Titanic\"])\n",
    "\n",
    "# filter the best model for each algorithm\n",
    "df_best = df[df.groupby(['params.algo'])['metrics.cv_score'].transform(max) == df['metrics.cv_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the best model for each algorithm\n",
    "estimators = []\n",
    "\n",
    "for index, row in df_best.iterrows():\n",
    "    print(row[\"params.algo\"])\n",
    "\n",
    "    if row[\"params.algo\"] == 'LGBMClassifier':\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=pd.to_numeric(row['params.lgb_n_estimators']),\n",
    "            learning_rate=pd.to_numeric(row['params.lgb_learning_rate']),\n",
    "            max_depth=pd.to_numeric(row['params.lgb_max_depth']),\n",
    "            num_leaves=pd.to_numeric(row['params.lgb_num_leaves']),\n",
    "            min_data_in_leaf=pd.to_numeric(row['params.lgb_min_data_in_leaf']),\n",
    "            subsample=pd.to_numeric(row['params.lgb_subsample']),\n",
    "            feature_fraction=pd.to_numeric(row['params.lgb_feature_fraction']),\n",
    "            reg_lambda=pd.to_numeric(row['params.lgb_reg_lambda']),\n",
    "            reg_alpha=pd.to_numeric(row['params.lgb_reg_alpha']),\n",
    "            num_boost_round  = 100\n",
    "        )\n",
    "\n",
    "    elif row[\"params.algo\"] == 'XGBClassifier':\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=pd.to_numeric(row['params.xgb_n_estimators']),\n",
    "            learning_rate=pd.to_numeric(row['params.xgb_learning_rate']),\n",
    "            reg_lambda=pd.to_numeric(row['params.xgb_reg_lambda']),\n",
    "            reg_alpha=pd.to_numeric(row['params.xgb_reg_alpha'])\n",
    "        )\n",
    "\n",
    "    elif row[\"params.algo\"] == 'DecisionTreeClassifier':\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=pd.to_numeric(row['params.dt_max_depth']),\n",
    "            criterion=row['params.dt_criterion'],\n",
    "            max_leaf_nodes=pd.to_numeric(row['params.dt_max_leaf_nodes']),\n",
    "        )\n",
    "\n",
    "    elif row[\"params.algo\"] == 'SVC':\n",
    "        model = SVC(\n",
    "            kernel=row['params.svm_kernel'],\n",
    "            C=pd.to_numeric(row['params.svm_C']),\n",
    "            degree=pd.to_numeric(row['params.svm_degree']),\n",
    "            probability=True\n",
    "        )\n",
    "\n",
    "    elif row[\"params.algo\"] == 'LogisticRegression':\n",
    "        model = LogisticRegression(\n",
    "            C=pd.to_numeric(row['params.lr_C']),\n",
    "            penalty=row['params.lr_penalty'],\n",
    "            solver=row['params.lr_solver'],\n",
    "        )\n",
    "\n",
    "    estimators.append((row[\"params.algo\"], model))\n",
    "\n",
    "\n",
    "# create the voting classifier\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "\n",
    "# train the voting classifier\n",
    "voting_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission\n",
    "create_submission(voting_clf, x_test, name=\"voting_clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30d15984fd22aa96de85f16433eec91bbe2faea5d46ed3d4d24713e4f4ec970c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
